{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "from peft.tuners.lora import Linear\n",
    "import torch.nn.functional as F\n",
    "from peft.utils.other import transpose\n",
    "from peft import PeftModel\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from blora import forward, StreamingPeftModel\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "Linear.forward = forward\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loras = [\"jondurbin/airoboros-7b-gpt4-1.2-peft\", \"trl-lib/llama-7b-se-rl-peft\", \"winddude/wizardLM-LlaMA-LoRA-7B\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:11<00:00,  2.80it/s]\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/ubuntu/llama-weights/7B/llama-7b\"\n",
    "\n",
    "torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "model = transformers.LlamaForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = transformers.LlamaTokenizer.from_pretrained(model_path)\n",
    "tokenizer.add_special_tokens({'pad_token': 'PAD'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_loras(model, loras):\n",
    "    # peft throws error if lora name contains a dot\n",
    "    adapters = [lora.replace(\".\", \"_\") for lora in loras]\n",
    "    lora_map = {lora: adapter for lora, adapter in zip(loras, adapters)}\n",
    "    model = PeftModel.from_pretrained(model, loras[0], adapter_name=adapters[0])\n",
    "    for lora, adapter in zip(loras[1:], adapters[1:]):\n",
    "        model = PeftModel.from_pretrained(model.base_model.model, lora, adapter_name=adapter)\n",
    "    return model, lora_map\n",
    "\n",
    "model, lora_map = load_loras(model, loras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Outline a five sentence short story where a character stumbles upon a secret room in their house that contains relics from their future.\",\n",
    "    \"Write a 6 line dialogue between a character and a magical creature that only they can see.\",\n",
    "    \"Describe a four sentence scene where a character discovers a hidden talent that changes their life forever.\",\n",
    "    \"Sculpt a three verse poem about the feeling of walking through a lush, vibrant garden in full bloom.\",\n",
    "    \"Develop an eight sentence short story about a character who can bring their dreams into reality, but only for a limited time.\",\n",
    "    \"Create a six sentence scene where a character finds themselves in a world where emotions are visible as colors surrounding each person.\",\n",
    "    \"Design an nine line dialogue between a character and a sentient cloud that follows them everywhere they go.\",\n",
    "    \"Narrate a 10 sentence story about a character who can switch between different realities, but can't control when or where they will end up.\",\n",
    "    \"Draft a three verse poem about the feeling of encountering a breathtaking view from a mountaintop.\",\n",
    "    \"Write a four sentence scene where a character discovers they can rewind time, but only in 10-second increments.\",\n",
    "    \"Capture a five sentence short story about a character who can communicate with nature, seeking help from plants and animals to solve a mystery.\",\n",
    "    \"Portray an eight line dialogue between a character and a ghost who is unaware of their own death.\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [(p, random.choice(loras)) for p in prompts]\n",
    "batch = tokenizer(prompts, return_tensors=\"pt\", padding=True)\n",
    "attention_masks = batch[\"input_ids\"].ne(tokenizer.pad_token_id).to(model.device)\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    module.batch_lora_ids = [lora_map[inp[1]] for inp in inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate(\n",
    "    input_ids=batch['input_ids'],\n",
    "    attention_mask=batch['attention_mask'],\n",
    "    max_length=200,\n",
    ")\n",
    "batch_decoded = tokenizer.batch_decode(torch.cat([out.reshape(-1, 1) for out in outputs], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'generator' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/minimal-llama/blora.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blambda-sft/home/ubuntu/minimal-llama/blora.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m out \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mgenerate(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blambda-sft/home/ubuntu/minimal-llama/blora.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     input_ids\u001b[39m=\u001b[39mbatch[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blambda-sft/home/ubuntu/minimal-llama/blora.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39mbatch[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blambda-sft/home/ubuntu/minimal-llama/blora.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     max_length\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blambda-sft/home/ubuntu/minimal-llama/blora.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m ):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blambda-sft/home/ubuntu/minimal-llama/blora.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     outputs\u001b[39m.\u001b[39mappend(out)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blambda-sft/home/ubuntu/minimal-llama/blora.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     batch_decoded \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_decode(torch\u001b[39m.\u001b[39mcat([out\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m) \u001b[39mfor\u001b[39;00m out \u001b[39min\u001b[39;00m outputs], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blambda-sft/home/ubuntu/minimal-llama/blora.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     clear_output(wait\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blambda-sft/home/ubuntu/minimal-llama/blora.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([lora \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m decoded \u001b[39mfor\u001b[39;00m lora, decoded \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(loras, batch_decoded)]))\n",
      "\u001b[1;32m/home/ubuntu/minimal-llama/blora.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blambda-sft/home/ubuntu/minimal-llama/blora.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m out \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mgenerate(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blambda-sft/home/ubuntu/minimal-llama/blora.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     input_ids\u001b[39m=\u001b[39mbatch[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blambda-sft/home/ubuntu/minimal-llama/blora.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39mbatch[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blambda-sft/home/ubuntu/minimal-llama/blora.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     max_length\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blambda-sft/home/ubuntu/minimal-llama/blora.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m ):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blambda-sft/home/ubuntu/minimal-llama/blora.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     outputs\u001b[39m.\u001b[39mappend(out)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blambda-sft/home/ubuntu/minimal-llama/blora.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     batch_decoded \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_decode(torch\u001b[39m.\u001b[39mcat([out\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m) \u001b[39mfor\u001b[39;00m out \u001b[39min\u001b[39;00m outputs], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blambda-sft/home/ubuntu/minimal-llama/blora.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     clear_output(wait\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blambda-sft/home/ubuntu/minimal-llama/blora.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([lora \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m decoded \u001b[39mfor\u001b[39;00m lora, decoded \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(loras, batch_decoded)]))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'generator' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "repetitions = 1\n",
    "timings=np.zeros((repetitions,1))\n",
    "\n",
    "outputs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for rep in range(repetitions):\n",
    "        starter.record()\n",
    "        for out in model.generate(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            max_length=200,\n",
    "        ):\n",
    "            outputs.append(out)\n",
    "            batch_decoded = tokenizer.batch_decode(torch.cat([out.reshape(-1, 1) for out in outputs], dim=1))\n",
    "            clear_output(wait=True)\n",
    "            print(\"\\n\\n\".join([lora + \":\\n\" + decoded for lora, decoded in zip(loras, batch_decoded)]))\n",
    "        ender.record()\n",
    "        torch.cuda.synchronize()\n",
    "        curr_time = starter.elapsed_time(ender)\n",
    "        timings[rep] = curr_time\n",
    "\n",
    "mean_syn = np.sum(timings) / repetitions\n",
    "std_syn = np.std(timings)\n",
    "print(mean_syn / 1000, std_syn / 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 4, 8, 16, 32, 64])\n",
    "y1 = np.array([14.13, 16.05, 22.43, 35.18, 62.25, 114.11, 219.20])\n",
    "y2 = 14.13 * x\n",
    "\n",
    "plt.plot(x, y1, 'b-')\n",
    "plt.plot(x, y2, 'r--')\n",
    "\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('Time (s)')\n",
    "\n",
    "plt.legend(['Batched Lora', 'Sequential'], loc='upper left')\n",
    "plt.title('Generating 200 tokens with Llama-7B using Batched Lora vs Sequential on A100-80gb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get slope of y1\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x,y1)\n",
    "print(slope)\n",
    "print(intercept)\n",
    "\n",
    "# which lib to import starts from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "9.7 + 3 * 3.3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
